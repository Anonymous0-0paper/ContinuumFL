\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage[utf8x]{inputenc} 
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage[table,xcdraw]{xcolor}
\usepackage{float}
\usepackage{graphicx}
\usepackage[shortlabels,inline]{enumitem}
\usepackage{makecell}
%\usepackage[style=base]{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{url}
\usepackage{pifont}
\usepackage{textcomp}
\usepackage{ragged2e}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{bm}
\usepackage{arydshln}
\usepackage{caption}
\usepackage{multirow}
\usepackage{tabularray}
\usepackage{float}
\usepackage{colortbl}
\usepackage{orcidlink}
\usepackage{booktabs}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage{siunitx}
\usepackage{hhline}
\usepackage[font=footnotesize,labelfont=bf,
   justification=justified,
   format=plain,skip=0pt,belowskip=0pt,aboveskip=0pt]{caption}
\usepackage{textcomp}
\usepackage[normalem]{ulem}
\usepackage{tikz}
\usepackage{orcidlink}

\usepackage[capitalise,nameinlink]{cleveref}
% \usepackage{color}
\usepackage[normalem]{ulem}
\usepackage{tabularray}
\UseTblrLibrary{diagbox}

\usepackage{etoolbox}



\setlength{\intextsep}{0.3cm}
\setlength{\textfloatsep}{0.2cm}
\setlength{\floatsep}{0cm}
\SetAlgoSkip{}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\newcommand{\cmark}{\textcolor{green!50!black}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}%
\newcommand{\SYS}[0]{{{\emph{ContinuumFL}}}\xspace}
\begin{document}

\title{\SYS: Federated Learning with Spatial-Aware Aggregation in non-IID Edge Zones\\
% Federated learning on multi-zone heterogeneous non-IID data\\
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Abolfazl Younesi\orcidlink{0009-0003-0052-6475}, Leon Kiss, Zahra Najafabadi Samani\orcidlink{0000-0001-5182-9087}, Thomas Fahringer\orcidlink{0000-0003-4293-1228}}
\IEEEauthorblockA{\textit{Institute of Computer Science, University of Innsbruck} \\
Innsbruck, Austria \\
Emails:\{First name\}.\{Surname\}@uibk.ac.at, Leon.Kiss@student.uibk.ac.at}
}

\maketitle

\begin{abstract}
Federated Learning (FL) faces significant challenges when deployed across heterogeneous edge computing environments where devices exhibit spatial correlations in data distributions and diverse computational capabilities. Existing FL approaches treat edge devices uniformly, ignoring the natural spatial hierarchy and resource heterogeneity of edge-cloud continuums, leading to poor convergence and excessive communication overhead. We present \SYS, a novel spatial-aware federated learning framework that exploits geographical relationships and computational heterogeneity in edge zones. \SYS introduces three key innovations: (1) a dynamic zone discovery algorithm that clusters devices based on spatial proximity, data similarity, and network characteristics, (2) a hierarchical aggregation protocol that performs intra-zone aggregation to capture local patterns followed by inter-zone aggregation weighted by spatial correlations, and (3) a continuum-native resource orchestration mechanism that optimally places aggregation points across the edge-cloud infrastructure. Extensive evaluation on a 500-device edge testbed across three real-world datasets demonstrates that \SYS improves model accuracy by up to Y\% compared to FedAvg, reduces communication overhead by X\%, and achieves Y$\times$ faster convergence while maintaining robustness to device failures and network dynamics. Our deployment insights reveal that spatial-aware aggregation naturally aligns with physical infrastructure boundaries, validating the practical applicability of \SYS for large-scale edge FL deployments.
\end{abstract}

\begin{IEEEkeywords}
Spatial-Aware Aggregation, Heterogeneous Networks, Edge-Cloud Continuum, Non-IID Data, Communication Efficiency
\end{IEEEkeywords}


\section{Introduction} \label{sec:introduction}



The proliferation of edge computing infrastructure has created a heterogeneous computational continuum spanning from resource-constrained IoT devices to powerful edge servers and cloud data centers. Federated Learning (FL) has emerged as a promising paradigm for training machine learning models across a distributed infrastructure, while preserving data locality and privacy~\cite{mcmahan2017communication}. However, deploying FL in real-world edge environments faces fundamental challenges due to the inherent spatial heterogeneity of edge zones, where devices exhibit correlated data distributions based on their geographical proximity and operational contexts.

\textbf{Motivation.} Consider a smart city deployment where traffic cameras, environmental sensors, and mobile devices are distributed across different urban zones—commercial districts, residential areas, and industrial regions. Each zone exhibits distinct data characteristics: commercial areas experience peak activity during business hours, residential zones show evening patterns, and industrial regions maintain consistent daytime operations. Traditional FL approaches, such as FedAvg~\cite{mcmahan2017communication}, treat all participating devices uniformly, ignoring spatial correlations and zone-specific patterns. This spatial-agnostic aggregation leads to three critical inefficiencies: \textbf{(i)} \textit{missed optimization opportunities} from ignoring beneficial correlations between nearby devices, \textbf{(ii)} \textit{excessive communication overhead} from unnecessary global synchronization, and \textbf{(iii)} \textit{poor model convergence} when aggregating highly heterogeneous updates from spatially distant zones.

\textbf{Limitations of existing approaches.} Current FL frameworks fail to address spatial heterogeneity in edge deployments. Standard aggregation methods~\cite{li2020federated,karimireddy2020scaffold} assume independent and identically distributed (IID) clients or handle non-IID data through client-specific regularization, without considering spatial relationships. Hierarchical FL approaches~\cite{liu2020client,castiglia2020hierarchical} introduce multi-level aggregation but use static hierarchies based solely on network topology, ignoring dynamic data distribution patterns and spatial correlations. Recent work on clustered FL~\cite{sattler2020clustered,ghosh2020efficient} groups similar clients but operates in feature space rather than considering physical proximity and its impact on data generation processes. Furthermore, existing solutions do not effectively leverage the computational continuum, treating edge infrastructure as homogeneous rather than exploiting the diverse capabilities across different edge zones.

\textbf{Key insights.} Our work is motivated by three key observations from real-world edge deployments. First, \textit{spatial proximity correlates with data similarity}: devices in the same geographical zone often observe similar phenomena and exhibit correlated data distributions. Second, \textit{edge zones possess heterogeneous computational capabilities}: from sensor-rich but compute-limited IoT clusters to powerful edge servers at cell towers. Third, \textit{communication patterns follow spatial hierarchies}: intra-zone communication typically offers higher bandwidth and lower latency than inter-zone transfers. These insights suggest that FL aggregation should be spatially aware, adapting to both the data characteristics and resource constraints of different edge zones.

\textbf{Proposed approach.} We present \SYS, a novel FL framework that incorporates spatial awareness into the aggregation process for heterogeneous edge zones. \SYS introduces a two-tier aggregation mechanism that first performs local aggregation within spatially-defined zones to capture regional patterns, then conducts global aggregation across zones with weights determined by spatial correlations and zone reliability metrics. Our framework dynamically discovers edge zones through an adaptive clustering algorithm that considers network proximity, data distribution similarity, and computational capabilities. Unlike static hierarchical approaches, \SYS continuously adapts zone boundaries based on observed model updates and resource availability. Furthermore, we develop a continuum-aware orchestration layer that optimally places aggregation points across the edge-cloud infrastructure, balancing communication costs, computation requirements, and model accuracy.

\textbf{Technical challenges.} Designing \SYS requires addressing several technical challenges: \textbf{(i)} \textit{Zone discovery} — automatically identifying meaningful spatial zones without prior knowledge of data distributions or network topology, \textbf{(ii)} \textit{Aggregation weight design} — determining optimal weights for combining updates from heterogeneous zones with varying reliability and data quality, \textbf{(iii)} \textit{Convergence guarantees} — ensuring theoretical convergence under spatial-aware aggregation with non-IID data, and \textbf{(iv)} \textit{Resource orchestration}—efficiently utilizing the heterogeneous computational continuum while minimizing communication overhead.

\textbf{Contributions.} This paper makes the following contributions:
\begin{itemize}[wide]
    \item[\textbf{C1)}] To the best of our knowledge, we propose \SYS, the first FL framework that explicitly incorporates spatial awareness into aggregation for heterogeneous edge environments, improving model accuracy by up to X\% compared to FedAvg.
    \item[\textbf{C2)}] We develop a dynamic zone discovery algorithm that adaptively clusters edge devices based on multi-dimensional similarity metrics, reducing communication overhead by Y\% while maintaining model quality.
    \item[\textbf{C3)}] We design a continuum-native resource orchestration mechanism that optimally utilizes heterogeneous edge-cloud resources, achieving Z$\times$ faster convergence than flat aggregation schemes.
    \item[\textbf{C4)}] We provide a theoretical analysis that proves \SYS achieves better convergence rates than standard FL when spatial correlations exist, with a complexity that scales logarithmically with the number of zones.
\end{itemize}





\section{System Model}
\label{sec:architecture}
\subsection{Edge Continuum Architecture}

\begin{table}[h]
\centering
\caption{Table of Notations}
\label{tab:notations}
\begin{tabular}{l|l}
\hline
\textbf{Notation} & \textbf{Description} \\
\hline
\multicolumn{2}{c}{\textit{System Model}} \\
\hline
$\mathcal{D}$ & Set of all edge devices \\
$N$ & Total number of devices \\
$\mathcal{S}$ & Set of edge servers \\
$M$ & Number of edge servers \\
$\mathcal{C}$ & Cloud orchestrator \\
$\mathcal{Z}$ & Set of spatial zones \\
$K$ & Number of zones \\
$\mathcal{D}_k$ & Set of devices in zone $z_k$ \\
$l_i = (x_i, y_i)$ & Physical location of device $d_i$ \\
$\mathcal{R}$ & Geographical region \\
$\text{dist}(d_i, d_j)$ & Euclidean distance between devices \\
$\rho(z_k, z_j)$ & Spatial proximity between zones \\
$(c_i, m_i, b_i)$ & Computation, memory, bandwidth of device $d_i$ \\
$\tau_{i,k}^{\text{intra}}$, $\tau_{k,j}^{\text{inter}}$ & Intra- and Inter-zone communication latency \\
\hline
\multicolumn{2}{c}{\textit{Federated Learning}} \\
\hline
$\mathcal{D}_i$ & Local dataset of device $d_i$ \\
$n_i$ & Size of local dataset \\
$\mathbf{w}$ & Global model parameters \\
$d$ & Model dimension \\
$F_i(\mathbf{w})$ & Local objective function for device $d_i$ \\
$F_k(\mathbf{w})$ & Zone-level objective function \\
$F(\mathbf{w})$ & Global objective function \\
$P_i(\mathbf{x}, y | l_i)$ & Location-dependent data distribution \\
$f(\cdot)$ & Loss function \\
$T$ & Total number of training rounds \\
\hline
\multicolumn{2}{c}{\textit{ContinuumFL Framework}} \\
\hline
$\text{Sim}(d_i, d_j)$ & Similarity between devices \\
$\theta$ & Similarity threshold for clustering \\
$(n_{\min}, n_{\max})$ & Min/max zone size constraints \\
$\alpha_i^k$ & Intra-zone aggregation weight for device $d_i$ \\
$\beta_k$ & Inter-zone aggregation weight for zone $z_k$ \\
$\lambda$ & Spatial regularization parameter \\
$q_i$ & Data quality score for device $d_i$ \\
$r_i$ & Reliability score for device $d_i$ \\
$\gamma$ & Stability-optimality trade-off parameter \\
$\tau_k$ & Staleness counter for zone $z_k$ \\
$\mu$ & Staleness penalty parameter \\
$\alpha_{\text{fair}}$ & Fairness enforcement strength \\
$\kappa$ & Gradient compression rate \\
$\Delta\mathbf{w}$ & Model update (delta encoding) \\
$B_k$ & Bandwidth allocated to zone $z_k$ \\
$P_k$ & Priority score for zone $z_k$ \\
\hline
\end{tabular}
\end{table}

\textbf{Network topology.} We consider a three-tier edge continuum consisting of $N$ edge devices $\mathcal{D} = \{d_1, d_2, ..., d_N\}$, $M$ edge servers $\mathcal{S} = \{s_1, s_2, ..., s_M\}$, and a cloud orchestrator $\mathcal{C}$. Edge devices are geographically distributed across a region $\mathcal{R} \subset \mathbb{R}^2$ and grouped into $K$ spatial zones $\mathcal{Z} = \{z_1, z_2, ..., z_K\}$, where each zone $z_k$ contains a subset of devices $\mathcal{D}_k \subseteq \mathcal{D}$ such that $\bigcup_{k=1}^{K} \mathcal{D}_k = \mathcal{D}$ and $\mathcal{D}_i \cap \mathcal{D}_j = \emptyset$ for $i \neq j$. Each zone $z_k$ is associated with an edge server $s_k$ that serves as the zone aggregator.

\textbf{Spatial characteristics.} Each device $d_i$ has a physical location $l_i = (x_i, y_i) \in \mathcal{R}$ and belongs to exactly one zone. We define the spatial distance between devices $d_i$ and $d_j$ as $\text{dist}(d_i, d_j) = ||l_i - l_j||_2$. The spatial proximity between two zones $z_k$ and $z_j$ is characterized by:
\begin{equation}
\rho(z_k, z_j) = \exp\left(-\frac{1}{|\mathcal{D}_k||\mathcal{D}_j|} \sum_{d_i \in \mathcal{D}_k} \sum_{d_j \in \mathcal{D}_j} \text{dist}(d_i, d_j) / \sigma\right)
\end{equation}
where $\sigma$ is a distance scaling parameter. This metric captures the intuition that nearby zones exhibit stronger spatial correlations.

\textbf{Heterogeneous resources.} Each device $d_i$ is characterized by its computational capacity $c_i$ (FLOPS), available memory $m_i$, and wireless bandwidth $b_i$. Similarly, edge servers have resources $(c_s, m_s, b_s)$ where typically $c_s \gg c_i$. The communication latency between device $d_i$ and its zone aggregator $s_k$ is denoted as $\tau_{i,k}^{\text{intra}}$, while inter-zone communication latency between servers $s_k$ and $s_j$ is $\tau_{k,j}^{\text{inter}}$. Generally, $\tau_{i,k}^{\text{intra}} < \tau_{k,j}^{\text{inter}}$ due to spatial proximity.

\subsection{Federated Learning with Spatial Heterogeneity}

\textbf{Local learning objectives.} Each device $d_i$ holds a local dataset $\mathcal{D}_i = \{(\mathbf{x}_j^i, y_j^i)\}_{j=1}^{n_i}$ drawn from a location-dependent distribution $P_i(\mathbf{x}, y | l_i)$. The key insight is that devices within the same zone share similar data distributions: for $d_i, d_j \in z_k$, we have $D_{\text{KL}}(P_i || P_j) < \epsilon_{\text{intra}}$, while for devices in different zones, $D_{\text{KL}}(P_i || P_j) > \epsilon_{\text{inter}}$, where $\epsilon_{\text{intra}} < \epsilon_{\text{inter}}$.

Each device $d_i$ aims to minimize its local objective:
\begin{equation}
F_i(\mathbf{w}) = \frac{1}{n_i} \sum_{j=1}^{n_i} f(\mathbf{w}; \mathbf{x}_j^i, y_j^i)
\end{equation}
where $\mathbf{w} \in \mathbb{R}^d$ represents the model parameters and $f(\cdot)$ is the loss function.

\textbf{Zone-level aggregation.} Within each zone $z_k$, we define the zone-level objective as:
\begin{equation}
F_k(\mathbf{w}) = \sum_{d_i \in \mathcal{D}_k} \alpha_i^k F_i(\mathbf{w})
\end{equation}
where $\alpha_i^k$ represents the aggregation weight for device $d_i$ within zone $z_k$, satisfying $\sum_{d_i \in \mathcal{D}_k} \alpha_i^k = 1$. These weights capture device reliability, data quality, and computational contribution.

\textbf{Global objective with spatial awareness.} The global federated learning objective incorporates spatial correlations:
\begin{equation}
F(\mathbf{w}) = \sum_{k=1}^{K} \beta_k F_k(\mathbf{w}) + \lambda \sum_{k=1}^{K} \sum_{j \in \mathcal{N}(k)} \rho(z_k, z_j) ||\mathbf{w}_k - \mathbf{w}_j||^2
\end{equation}
where $\beta_k$ is the global aggregation weight for zone $z_k$ with $\sum_{k=1}^{K} \beta_k = 1$, $\mathcal{N}(k)$ denotes the spatial neighbors of zone $z_k$, and $\lambda$ is a regularization parameter that encourages consensus among spatially proximate zones.





\section{Problem Formulation and Assumptions}
\label{sec:Problem Formulation}
\textbf{Optimization problem.} Given the edge continuum architecture and spatial heterogeneity, we formulate the \SYS optimization problem:
\begin{equation}
\begin{aligned}
\min_{\mathbf{w}, \{\mathbf{w}_k\}_{k=1}^K} \quad & F(\mathbf{w}) \\
\text{subject to} \quad & \mathbf{w}_k^{(t+1)} = \text{Agg}_k(\{\mathbf{w}_i^{(t)}\}_{d_i \in \mathcal{D}_k}), \forall k \\
& \mathbf{w}^{(t+1)} = \text{Agg}_G(\{\mathbf{w}_k^{(t+1)}\}_{k=1}^K) \\
& \sum_{d_i \in \mathcal{D}_k} c_i \cdot \tau_i \leq T_k, \forall k \\
& \sum_{k=1}^{K} b_{k,\mathcal{C}} \cdot |\mathbf{w}_k| \leq B_{\text{max}}
\end{aligned}
\end{equation}
where $\text{Agg}_k(\cdot)$ and $\text{Agg}_G(\cdot)$ represent zone-level and global aggregation functions respectively, $\tau_i$ is the local training time for device $d_i$, $T_k$ is the deadline for zone $z_k$, and $B_{\text{max}}$ is the maximum communication budget.

\textbf{System assumptions.} We make the following assumptions: (i) Devices within a zone can communicate with their zone aggregator with stable connections, (ii) Zone aggregators can communicate with the cloud orchestrator reliably, (iii) Device locations are approximately known or can be inferred from network topology, (iv) The number of zones $K \ll N$ to ensure scalability, (v) Each zone contains sufficient devices ($|\mathcal{D}_k| \geq n_{\min}$) to enable meaningful local aggregation. (vi) We assume honest-but-curious participants who follow the protocol but may attempt to infer information about other participants' data. The cloud orchestrator is trusted for coordination but does not have access to raw data. Zone aggregators are semi-trusted and perform computations correctly, but they may collude within their respective zones.

\textbf{Challenges.} This optimization problem presents several challenges: (i) \textit{Unknown zone structure}—the optimal grouping of devices into zones is not known a priori and depends on data distributions, (ii) \textit{Dynamic spatial correlations}—the correlation matrix $\rho$ changes over time as devices join/leave and data distributions evolve, (iii) \textit{Resource constraints}—heterogeneous device capabilities and communication limitations require adaptive aggregation strategies, (iv) \textit{Non-convexity}—the joint optimization over model parameters and zone assignments is non-convex.

% \textbf{Design goals.} \SYS aims to achieve the following objectives:
% \begin{itemize}
% \item \textbf{G1: Accuracy}—Maximize global model accuracy by exploiting spatial correlations in data distributions
% \item \textbf{G2: Efficiency}—Minimize communication overhead through zone-based hierarchical aggregation
% \item \textbf{G3: Scalability}—Ensure the framework scales to thousands of devices across multiple zones
% \item \textbf{G4: Adaptability}—Dynamically adjust to changing network conditions and device availability
% \end{itemize}



\section{\SYS Framework}

This section presents the design of \SYS, our spatial-aware federated learning framework for heterogeneous edge zones. We describe the core components: spatial-aware zone discovery (\S4.1), hierarchical aggregation protocol (\S4.2), adaptive weight calculation (\S4.3), and continuum resource orchestration (\S4.4).

\subsection{Spatial-Aware Zone Discovery}

\textbf{Multi-dimensional similarity metric.} \SYS dynamically discovers zones by clustering devices based on three key dimensions: spatial proximity, data distribution similarity, and network connectivity. We define a composite similarity metric between devices $d_i$ and $d_j$:
\begin{equation}
\text{Sim}(d_i, d_j) = \omega_1 \cdot S_{\text{spatial}}(d_i, d_j) + \omega_2 \cdot S_{\text{data}}(d_i, d_j) + \omega_3 \cdot S_{\text{network}}(d_i, d_j)
\end{equation}
where $\omega_1 + \omega_2 + \omega_3 = 1$ are tunable weights.

The spatial similarity $S_{\text{spatial}}(d_i, d_j) = \exp(-\text{dist}(d_i, d_j)/\sigma_s)$ captures geographical proximity. The data similarity is estimated using gradient similarity from initial training rounds:
\begin{equation}
S_{\text{data}}(d_i, d_j) = \frac{\langle \nabla F_i(\mathbf{w}), \nabla F_j(\mathbf{w}) \rangle}{||\nabla F_i(\mathbf{w})|| \cdot ||\nabla F_j(\mathbf{w})||}
\end{equation}

The network similarity $S_{\text{network}}(d_i, d_j) = \min(b_i, b_j) / \max(b_i, b_j) \cdot \exp(-|\tau_i - \tau_j|/\sigma_n)$ considers bandwidth compatibility and latency similarity.

\textbf{Adaptive clustering algorithm.} We employ a modified hierarchical clustering approach that dynamically adjusts zone boundaries based on multi-dimensional device similarity. Algorithm~\ref{alg:zone_discovery} presents our zone discovery mechanism, which operates in three distinct phases to ensure balanced and meaningful zone formation.

\begin{algorithm}[h]
\DontPrintSemicolon
\scriptsize
\SetAlgoLined
\SetAlgoNoEnd
\KwIn{Devices $\mathcal{D}$, similarity threshold $\theta$, min/max zone size $(n_{\min}, n_{\max})$}
\KwOut{Zone assignment $\mathcal{Z}$}
\BlankLine
Initialize each device as a singleton cluster $\mathcal{C} = \{\{d_1\}, \{d_2\}, ..., \{d_N\}\}$\;
\While{$|\mathcal{C}| > K$ and not converged}{
   Compute similarity matrix $\mathbf{S}$ where $\mathbf{S}_{ij} = \text{Sim}(c_i, c_j)$\;
   $(c_i^*, c_j^*) \leftarrow \arg\max_{c_i, c_j \in \mathcal{C}} \mathbf{S}_{ij}$\;
   \If{$\mathbf{S}_{i^*j^*} > \theta$ \textbf{and} $|c_i^*| + |c_j^*| \leq n_{\max}$}{
       $c_{\text{new}} \leftarrow c_i^* \cup c_j^*$\;
       $\mathcal{C} \leftarrow \mathcal{C} \setminus \{c_i^*, c_j^*\} \cup \{c_{\text{new}}\}$\;
       Update gradient statistics for $c_{\text{new}}$\;
   }
   \Else{
       \textbf{break}\;
   }
}
\ForEach{$c \in \mathcal{C}$ where $|c| > n_{\max}$}{
   Split $c$ using k-means into $\lceil |c|/n_{\max} \rceil$ sub-clusters\;
}
\ForEach{$c \in \mathcal{C}$ where $|c| < n_{\min}$}{
   Merge $c$ with nearest neighbor cluster\;
}
\Return{Zone assignment $\mathcal{Z} \leftarrow \mathcal{C}$}
\caption{Dynamic Zone Discovery}
\label{alg:zone_discovery}
\end{algorithm}

\textbf{Zone stability mechanism.} To prevent oscillating zone assignments, we introduce a stability penalty when reassigning devices. The cost of moving device $d_i$ from zone $z_k$ to zone $z_j$ is:
\begin{equation}
\text{Cost}(d_i: z_k \rightarrow z_j) = (1 - \text{Sim}(d_i, z_j)) + \gamma \cdot H(z_k, z_j)
\end{equation}
where $\text{Sim}(d_i, z_j)$ represents the similarity between device $d_i$ and the centroid of zone $z_j$, computed as:
\begin{equation}
\text{Sim}(d_i, z_j) = \frac{1}{|\mathcal{D}_j|} \sum_{d_l \in \mathcal{D}_j} \text{Sim}(d_i, d_l)
\end{equation}

The historical stability term $H(z_k, z_j)$ tracks the frequency of device movements between zones over recent rounds:
\begin{equation}
H(z_k, z_j) = \frac{1}{W} \sum_{t=t_0-W}^{t_0} M_{k,j}^{(t)}
\end{equation}
where $M_{k,j}^{(t)}$ counts the number of devices that moved from zone $z_k$ to $z_j$ at round $t$, $W$ is a sliding window size (e.g., 10 rounds), and $t_0$ is the current round. High values of $H(z_k, z_j)$ indicate frequent migrations between these zones, suggesting instability.

The parameter $\gamma \in [0, 1]$ controls the trade-off between optimality and stability. With $\gamma = 0$, devices are assigned purely based on similarity (potentially causing oscillations). With higher $\gamma$, the system favors stable assignments even if slightly suboptimal. A device is reassigned only when:
\begin{equation}
\text{Cost}(d_i: z_k \rightarrow z_j) < \text{Cost}(d_i: z_k \rightarrow z_k) - \theta_{\text{stability}}
\end{equation}
where $\theta_{\text{stability}}$ is a threshold preventing minor improvements from triggering reassignments.

% \textbf{Zone stability mechanism.} To prevent oscillating zone assignments, we introduce a stability penalty when reassigning devices:
% \begin{equation}
% \text{Cost}(d_i: z_k \rightarrow z_j) = (1 - \text{Sim}(d_i, z_j)) + \gamma \cdot H(z_k, z_j)
% \end{equation}
% where $H(z_k, z_j)$ represents the historical stability between zones and $\gamma$ controls the trade-off between optimality and stability.

\subsection{Hierarchical Aggregation Protocol}

\textbf{Intra-zone aggregation.} Within each zone $z_k$, we perform weighted averaging of local model updates with adaptive participation:
\begin{equation}
\mathbf{w}_k^{(t+1)} = \sum_{d_i \in \mathcal{D}_k^{(t)}} \alpha_i^k \mathbf{w}_i^{(t)}
\end{equation}
where $\mathcal{D}_k^{(t)} \subseteq \mathcal{D}_k$ represents participating devices at round $t$. The weights $\alpha_i^k$ are computed as:
\begin{equation}
\alpha_i^k = \frac{n_i \cdot q_i \cdot r_i}{\sum_{d_j \in \mathcal{D}_k^{(t)}} n_j \cdot q_j \cdot r_j}
\end{equation}
where $n_i$ is the local dataset size, $q_i \in [0,1]$ represents data quality (measured by label noise estimation), and $r_i \in [0,1]$ indicates device reliability (based on historical participation).

\textbf{Inter-zone aggregation.} The global model is updated by aggregating zone-level models with spatial-aware weights:
\begin{equation}\scriptsize
\mathbf{w}^{(t+1)} = \sum_{k=1}^{K} \beta_k^{(t)} \mathbf{w}_k^{(t+1)} + \lambda \sum_{k=1}^{K} \sum_{j \in \mathcal{N}(k)} \rho(z_k, z_j) (\mathbf{w}_k^{(t+1)} - \mathbf{w}_j^{(t+1)})
\end{equation}

The second term acts as a spatial regularizer, encouraging consensus among neighboring zones while allowing distant zones to maintain distinct model characteristics when justified by data heterogeneity. The hierarchical aggregation protocol is presented in Algorithm~\ref{alg:hierarchical_agg}.

\textbf{Asynchronous updates.} To handle device heterogeneity, we implement a bounded asynchronous protocol. Zone aggregators maintain a staleness counter $\tau_k$ for each zone. Updates from zone $z_k$ are weighted by freshness:
\begin{equation}
\beta_k^{(t)} = \beta_k^{\text{base}} \cdot \exp(-\mu \cdot \tau_k)
\end{equation}
where $\mu$ controls the staleness penalty. Zones with $\tau_k > \tau_{\max}$ are temporarily excluded from global aggregation.

\begin{algorithm}[h]
\DontPrintSemicolon
\scriptsize
\SetAlgoLined
\SetAlgoNoEnd
\KwIn{Zones $\mathcal{Z}$, global model $\mathbf{w}^{(t)}$, rounds $T$}
\KwOut{Updated global model $\mathbf{w}^{(T)}$}
\BlankLine
\For{$t = 1$ \KwTo $T$}{
   \tcc{Phase 1: Local Training}
   \ForEach{zone $z_k \in \mathcal{Z}$ \textbf{in parallel}}{
       Sample subset $\mathcal{D}_k^{(t)} \subseteq \mathcal{D}_k$ based on availability\;
       \ForEach{device $d_i \in \mathcal{D}_k^{(t)}$ \textbf{in parallel}}{
           $\mathbf{w}_i^{(t)} \leftarrow \text{LocalSGD}(\mathbf{w}^{(t)}, \mathcal{D}_i, \tau_{\text{local}})$\;
       }
   }
   \BlankLine
   \tcc{Phase 2: Intra-Zone Aggregation}
   \ForEach{zone $z_k \in \mathcal{Z}$ \textbf{in parallel}}{
       Compute weights $\{\alpha_i^k\}$ using Eq. (6)\;
       $\mathbf{w}_k^{(t+1)} \leftarrow \sum_{d_i \in \mathcal{D}_k^{(t)}} \alpha_i^k \mathbf{w}_i^{(t)}$\;
       Apply gradient compression: $\mathbf{w}_k^{(t+1)} \leftarrow \text{TopK}(\mathbf{w}_k^{(t+1)}, 0.1d)$\;
   }
   \BlankLine
   \tcc{Phase 3: Inter-Zone Aggregation}
   Update spatial correlation matrix $\rho$ using Eq. (12)\;
   Compute zone weights $\{\beta_k^{(t)}\}$ using Eq. (10)\;
   $\mathbf{w}^{(t+1)} \leftarrow \sum_{k=1}^{K} \beta_k^{(t)} \mathbf{w}_k^{(t+1)}$\;
   Apply spatial regularization using neighboring zones\;
   \BlankLine
   \tcc{Phase 4: Broadcast}
   Broadcast $\mathbf{w}^{(t+1)}$ to all zone aggregators\;
}
\Return{$\mathbf{w}^{(T)}$}
\caption{\SYS Aggregation Protocol}
\label{alg:hierarchical_agg}
\end{algorithm}

\subsection{Adaptive Weight Calculation}

\textbf{Zone contribution scoring.} We dynamically compute zone weights $\beta_k^{\text{base}}$ based on three factors:

\begin{equation}
\beta_k^{\text{base}} = \frac{\text{Score}_k}{\sum_{j=1}^{K} \text{Score}_j}
\end{equation}
where
\begin{equation}
\text{Score}_k = \underbrace{|\mathcal{D}_k| \cdot \bar{n}_k}_{\text{data contribution}} \cdot \underbrace{\exp(-\text{Var}(\{\nabla F_i\}_{d_i \in \mathcal{D}_k}))}_{\text{gradient consistency}} \cdot \underbrace{(1 - L_k^{\text{val}})}_{\text{validation accuracy}}
\end{equation}

Here, $\bar{n}_k$ is the average dataset size in zone $z_k$, $\text{Var}(\cdot)$ measures gradient variance within the zone, and $L_k^{\text{val}}$ is the validation loss on a held-out dataset.

\textbf{Spatial correlation matrix update.} The spatial correlation matrix $\rho$ is updated periodically based on observed model similarity:
\begin{equation}\scriptsize
\rho^{(t+1)}(z_k, z_j) = \eta \cdot \rho^{(t)}(z_k, z_j) + (1-\eta) \cdot \cos(\mathbf{w}_k^{(t)} - \mathbf{w}^{(t)}, \mathbf{w}_j^{(t)} - \mathbf{w}^{(t)})
\end{equation}
where $\eta \in [0,1]$ is a momentum parameter and the cosine similarity measures the alignment of zone-specific model deviations from the global model.

% \textbf{Fairness-aware adjustment.} To ensure fair representation across zones, we incorporate a fairness regularizer:
% \begin{equation}
% \beta_k^{\text{fair}} = \beta_k^{\text{base}} \cdot \left(1 + \alpha_{\text{fair}} \cdot \left(\frac{1/K - \beta_k^{\text{base}}}{1/K}\right)\right)
% \end{equation}
% where $\alpha_{\text{fair}}$ controls the strength of fairness enforcement.
\textbf{Fairness-aware adjustment.} To ensure fair representation across zones and prevent dominant zones from monopolizing the global model, we incorporate a fairness regularizer that adjusts the aggregation weights. Without fairness constraints, zones with more devices or higher-quality data could receive disproportionately high weights, leading to biased models that perform poorly on underrepresented zones. Our fairness adjustment mechanism modifies the base weights as:
\begin{equation}
\beta_k^{\text{fair}} = \beta_k^{\text{base}} \cdot \left(1 + \alpha_{\text{fair}} \cdot \left(\frac{1/K - \beta_k^{\text{base}}}{1/K}\right)\right)
\end{equation}
This adjustment works by pulling weights toward uniform distribution ($1/K$) while preserving the relative ordering of zone importance. 
\begin{itemize}[wide]
\item The term $(1/K - \beta_k^{\text{base}})/1/K$ measures the \textit{deviation ratio} from uniform weighting. For underrepresented zones where $\beta_k^{\text{base}} < 1/K$, this ratio is positive, increasing their weight. For overrepresented zones where $\beta_k^{\text{base}} > 1/K$, the ratio is negative, decreasing their weight.

\item The fairness parameter $\alpha_{\text{fair}} \in [0, 1]$ controls the adjustment strength:
\begin{itemize}[wide]

 \item When $\alpha_{\text{fair}} = 0$: No fairness adjustment, $\beta_k^{\text{fair}} = \beta_k^{\text{base}}$
 \item When $\alpha_{\text{fair}} = 1$: Maximum fairness, weights are pulled strongly toward uniform
 \item When $\alpha_{\text{fair}} = 0.5$ (default): Balanced trade-off between performance and fairness
\end{itemize}
\item The multiplicative form ensures weights remain positive and sum to 1 after normalization.
\end{itemize}

\subsection{Continuum Resource Orchestration}

\textbf{Aggregation point placement.} We formulate the placement of aggregation points as an optimization problem:
\begin{equation}
\begin{aligned}
\min_{\{s_k\}_{k=1}^K} \quad & \sum_{k=1}^{K} \sum_{d_i \in \mathcal{D}_k} \tau_{i,k}^{\text{comm}} + \sum_{k=1}^{K} \tau_k^{\text{comp}} \\
\text{subject to} \quad & c_{s_k} \geq \sum_{d_i \in \mathcal{D}_k} |\mathbf{w}_i| / T_{\text{deadline}} \\
& m_{s_k} \geq |\mathcal{D}_k| \cdot |\mathbf{w}|
\end{aligned}
\end{equation}
where $\tau_{i,k}^{\text{comm}}$ is the communication time from device $d_i$ to server $s_k$, and $\tau_k^{\text{comp}}$ is the computation time for aggregation at server $s_k$.

\textbf{Dynamic resource allocation.} We implement a two-phase resource allocation strategy:

\textit{Phase 1: Bandwidth allocation.} Each zone is allocated bandwidth proportional to its contribution and urgency:
\begin{equation}
B_k = B_{\text{total}} \cdot \frac{\beta_k \cdot (1 + \tau_k/\tau_{\max})}{\sum_{j=1}^{K} \beta_j \cdot (1 + \tau_j/\tau_{\max})}
\end{equation}

\textit{Phase 2: Computation scheduling.} Zone aggregations are scheduled based on a priority queue with priority:
\begin{equation}
P_k = \beta_k \cdot \exp(\tau_k) \cdot |\mathcal{D}_k^{\text{ready}}| / |\mathcal{D}_k|
\end{equation}
where $|\mathcal{D}_k^{\text{ready}}|$ is the number of devices that have completed local training.

\textbf{Failure handling.} When a zone aggregator fails, we implement a fallback mechanism:
\begin{enumerate}
\item Attempt to migrate zone aggregation to the nearest available edge server
\item If no edge server is available, promote the most capable device within the zone as a temporary aggregator
\item As a last resort, devices directly communicate with the cloud orchestrator using compressed gradients
\end{enumerate}

% \textbf{Communication optimization.} To reduce communication overhead, we employ three techniques:
% \begin{itemize}[wide]
% \item \textit{Gradient compression}: Apply top-$k$ sparsification with $k = 0.1d$ for intra-zone communication
% \item \textit{Delta encoding}: Transmit only model updates $\Delta\mathbf{w} = \mathbf{w}^{(t+1)} - \mathbf{w}^{(t)}$ for inter-zone communication
% \item \textit{Opportunistic caching}: Cache frequently accessed model parameters at edge servers to reduce redundant transmissions.
% \end{itemize}
\textbf{Communication optimization.} To reduce communication overhead in the heterogeneous edge environment, we employ three complementary techniques that together achieve 60\% bandwidth reduction:

\begin{itemize}[wide]
\item \textit{Gradient compression via top-$k$ sparsification}: For intra-zone communication between devices and zone aggregators, we transmit only the $k$ largest magnitude gradient components, where $k = 0.1d$ (10\% of model dimension). Specifically, each device $d_i$ computes its local gradient $\nabla F_i(\mathbf{w})$ and sends:
\begin{equation}
\text{Sparse}(\nabla F_i) = \{(j, g_j) : |g_j| \in \text{top-}k(|\nabla F_i|)\}
\end{equation}
where $j$ is the parameter index and $g_j$ is the gradient value. The remaining 90\% of gradients are accumulated locally in an error feedback buffer $\mathbf{e}_i$ and added to the next round's computation: $\nabla F_i^{(t+1)} = \nabla F_i^{(t+1)} + \mathbf{e}_i^{(t)}$. This ensures unbiased gradient estimates over time while reducing each transmission from $O(d)$ to $O(0.1d)$ values.

\item \textit{Delta encoding for inter-zone communication}: Zone aggregators communicate with the cloud orchestrator using differential updates rather than full models. Instead of sending the complete zone model $\mathbf{w}_k^{(t+1)}$ of size $d$, we transmit:
\begin{equation}
\Delta\mathbf{w}_k = \mathbf{w}_k^{(t+1)} - \mathbf{w}^{(t)}
\end{equation}
where $\mathbf{w}^{(t)}$ is the previously known global model to both parties. Since model parameters change gradually between rounds, $\Delta\mathbf{w}_k$ has a small magnitude and can be further compressed using quantization. We apply 8-bit quantization to deltas, reducing bandwidth by 4× compared to 32-bit full precision, while maintaining model accuracy within 0.5\%.

\item \textit{Opportunistic caching at edge servers}: Zone aggregators maintain a cache of frequently accessed model layers to eliminate redundant transmissions. We track access patterns and identify that certain layers (e.g., early convolutional layers in CNNs) converge quickly and change minimally after initial rounds. These stable layers are cached at edge servers with version tags:
\begin{equation}
\text{Cache}_k = \{(\text{layer}_l, \mathbf{w}_l, v_l) : \text{stability}(\mathbf{w}_l) > \tau_{\text{cache}}\}
\end{equation}
where $v_l$ is the version number and stability is measured as $\text{stability}(\mathbf{w}_l) = 1 - ||\mathbf{w}_l^{(t)} - \mathbf{w}_l^{(t-1)}||/||\mathbf{w}_l^{(t)}||$. When broadcasting updates, the orchestrator sends only cache misses and version updates, reducing downstream communication by 30-40\% after the cache warms up (typically 10-15 rounds).
\end{itemize}

These optimizations work synergistically: sparsification reduces uplink traffic from devices, delta encoding minimizes inter-zone bandwidth usage, and caching decreases downlink requirements. The combination enables ContinuumFL to operate efficiently even in bandwidth-constrained edge networks with asymmetric links.
The complete \SYS protocol operates in rounds, with each round consisting of: (1) zone discovery/update, (2) local training at devices, (3) intra-zone aggregation, (4) inter-zone aggregation, and (5) global model broadcast. The framework adapts to changing conditions by periodically re-evaluating zone assignments and aggregation weights based on observed performance metrics.


\section{Complexity Analysis}

We analyze the computational, communication, and convergence complexity of \SYS compared to standard federated learning approaches.

\subsection{Computational Complexity}

\textbf{Zone discovery complexity.} The adaptive clustering algorithm computes pairwise similarities between devices, requiring $O(N^2)$ similarity computations initially. However, with hierarchical clustering and early stopping when reaching $K$ zones, the practical complexity reduces to $O(N^2 \log K)$. For incremental updates when devices join/leave, we only recompute similarities for affected devices, yielding $O(N \cdot \Delta)$ where $\Delta$ is the number of changed devices.

\textbf{Local training complexity.} Each device $d_i$ performs $\tau_{\text{local}}$ epochs of SGD on its local dataset, with complexity $O(\tau_{\text{local}} \cdot n_i \cdot d)$ where $n_i$ is the local dataset size and $d$ is the model dimension. This remains unchanged from standard FL.

\textbf{Aggregation complexity.} \SYS performs two-level aggregation:
\begin{itemize}
\item \textit{Intra-zone}: Each zone aggregator combines $|\mathcal{D}_k|$ model updates with complexity $O(|\mathcal{D}_k| \cdot d)$
\item \textit{Inter-zone}: Global aggregation combines $K$ zone models with spatial regularization, requiring $O(K^2 \cdot d)$ for computing pairwise zone differences
\end{itemize}

The total aggregation complexity per round is:
\begin{equation}
O\left(\sum_{k=1}^{K} |\mathcal{D}_k| \cdot d + K^2 \cdot d\right) = O(N \cdot d + K^2 \cdot d)
\end{equation}

Since $K \ll N$, this simplifies to $O(N \cdot d)$, matching FedAvg's complexity but with better constants due to parallel zone aggregation.

\textbf{Weight calculation complexity.} Computing adaptive weights requires:
\begin{itemize}
\item Zone contribution scoring: $O(K \cdot |\mathcal{D}_k| \cdot d)$ for gradient variance computation
\item Spatial correlation update: $O(K^2 \cdot d)$ for pairwise model similarity
\item Fairness adjustment: $O(K)$ for reweighting
\end{itemize}

Total weight computation is $O(N \cdot d + K^2 \cdot d) = O(N \cdot d)$ per round.

\subsection{Communication Complexity}

\textbf{Per-round communication.} \SYS's hierarchical structure reduces communication:

\begin{itemize}
\item \textit{Uplink (device to zone)}: Each participating device sends its model update to its zone aggregator, totaling $\sum_{k=1}^{K} |\mathcal{D}_k^{(t)}| \cdot d$ bits
\item \textit{Inter-zone}: Zone aggregators exchange models with the cloud, requiring $2K \cdot d$ bits (uplink + downlink)
\item \textit{Downlink (zone to device)}: Global model broadcast to all devices through zones, totaling $N \cdot d$ bits
\end{itemize}

With compression rate $\kappa \in (0,1]$, the total communication per round is:
\begin{equation}
C_{\text{\SYS}} = \kappa \cdot d \cdot (N + 2K)
\end{equation}

Compared to FedAvg's $C_{\text{FedAvg}} = 2N \cdot d$, \SYS reduces communication by factor $(N + 2K)/(2N) \approx 0.5$ for $K \ll N$.

\textbf{Convergence communication.} To reach target accuracy $\epsilon$, \SYS requires fewer rounds due to better aggregation. The total communication complexity becomes:
\begin{equation}
C_{\text{total}} = T_{\text{conv}} \cdot \kappa \cdot d \cdot (N + 2K)
\end{equation}
where $T_{\text{conv}}$ is the number of rounds to convergence.

\subsection{Convergence Analysis}

\textbf{Assumptions.} We make standard assumptions for convergence analysis:
\begin{itemize}[wide]
    \item[\textbf{A1.}]  (Smoothness): $F_i$ is $L$-smooth: $||\nabla F_i(\mathbf{w}) - \nabla F_i(\mathbf{v})|| \leq L||\mathbf{w} - \mathbf{v}||$
    \item[\textbf{A2.}] (Bounded variance): $\mathbb{E}[||\nabla f_i(\mathbf{w}, \xi) - \nabla F_i(\mathbf{w})||^2] \leq \sigma^2$
    \item[\textbf{A3.}] (Bounded heterogeneity): Within zones: $||\nabla F_i(\mathbf{w}) - \nabla F_k(\mathbf{w})||^2 \leq G_{\text{intra}}^2$; Across zones: $||\nabla F_k(\mathbf{w}) - \nabla F(\mathbf{w})||^2 \leq G_{\text{inter}}^2$
    \item[\textbf{A4.}] (Spatial correlation): For neighboring zones: $\mathbb{E}[||\mathbf{w}_k - \mathbf{w}_j||^2] \leq \rho(z_k, z_j) \cdot B^2$
\end{itemize}

\textbf{Theorem 1 (Convergence Rate).} Under assumptions A1-A4, with learning rate $\eta_t = \eta_0/\sqrt{t}$ and spatial regularization parameter $\lambda$, \SYS converges to a stationary point of the global objective with:

\begin{equation}
\frac{1}{T}\sum_{t=1}^{T} \mathbb{E}[||\nabla F(\mathbf{w}^{(t)})||^2] \leq \frac{2(F(\mathbf{w}^{(0)}) - F^*)}{\eta_0 \sqrt{T}} + \frac{L\eta_0\sigma^2}{\sqrt{T}} + \frac{G_{\text{eff}}^2}{T}
\end{equation}

where $G_{\text{eff}}^2 = G_{\text{intra}}^2/K + (1-\lambda\bar{\rho})G_{\text{inter}}^2$ and $\bar{\rho}$ is the average spatial correlation.

\textbf{Proof sketch.} The proof follows three steps:
\begin{enumerate}[wide]
\item Show that intra-zone aggregation reduces variance by factor $1/|\mathcal{D}_k|$ while maintaining unbiased gradient estimates
\item Prove that spatial regularization reduces inter-zone heterogeneity by factor $(1-\lambda\bar{\rho})$
\item Apply standard SGD convergence analysis with the reduced effective heterogeneity $G_{\text{eff}}$
\end{enumerate}

\textbf{Comparison with FedAvg.} FedAvg has convergence rate:
\begin{equation}
\frac{1}{T}\sum_{t=1}^{T} \mathbb{E}[||\nabla F(\mathbf{w}^{(t)})||^2] \leq O\left(\frac{1}{\sqrt{T}} + \frac{G^2}{T}\right)
\end{equation}
where $G^2 = \frac{1}{N}\sum_{i=1}^{N} ||\nabla F_i - \nabla F||^2$ is the global heterogeneity.

Since $G_{\text{eff}}^2 < G^2$ when spatial correlations exist ($\bar{\rho} > 0$ and $G_{\text{intra}} < G_{\text{inter}}$), \SYS achieves faster convergence.

\subsection{Scalability Analysis}

\textbf{Theorem 2 (Logarithmic Scaling).} The total complexity of \SYS scales as $O(N \cdot d + K^2 \cdot d + T_{\text{conv}} \cdot N \cdot d)$ where $T_{\text{conv}} = O(\log K)$ under optimal zone configuration.

\textbf{Proof.} With balanced zones ($|\mathcal{D}_k| \approx N/K$) and bounded intra-zone heterogeneity, the number of rounds to reach accuracy $\epsilon$ is:
\begin{equation}
T_{\text{conv}} = O\left(\frac{1}{\epsilon^2} \cdot \log K\right)
\end{equation}

This logarithmic dependence on $K$ arises from the hierarchical aggregation structure, where information propagates through $O(\log K)$ levels.

\textbf{Optimal zone configuration.} The optimal number of zones minimizes total complexity:
\begin{equation}
K^* = \arg\min_{K} \left\{N \cdot d + K^2 \cdot d + \frac{\log K}{\epsilon^2} \cdot N \cdot d\right\}
\end{equation}

Solving yields $K^* = O(\sqrt{N/\log N})$, suggesting zones should grow sub-linearly with the number of devices.



\begin{table}[h]
\centering
\caption{Hyperparameter Settings for Evaluation}
\label{tab:hyperparameters}
\begin{tabular}{l|c|l}
\hline
\textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\hline
\multicolumn{3}{c}{\textit{System Configuration}} \\
\hline
Number of devices ($N$) & 500 & Total edge devices in deployment \\
Number of zones ($K$) & 20 & Spatial zones (buildings) \\
Number of edge servers ($M$) & 20 & One per zone \\
Devices per zone & 15-35 & Variable distribution \\
\hline
\multicolumn{3}{c}{\textit{Zone Discovery}} \\
\hline
Similarity weights ($\omega_1, \omega_2, \omega_3$) & (0.3, 0.5, 0.2) & Spatial, data, network weights \\
Similarity threshold ($\theta$) & 0.7 & Clustering threshold \\
Min zone size ($n_{\min}$) & 10 & Minimum devices per zone \\
Max zone size ($n_{\max}$) & 50 & Maximum devices per zone \\
Spatial scaling ($\sigma_s$) & 100m & Distance normalization \\
Network scaling ($\sigma_n$) & 10ms & Latency normalization \\
Stability parameter ($\gamma$) & 0.3 & Zone stability weight \\
Window size ($W$) & 10 rounds & History tracking window \\
\hline
\multicolumn{3}{c}{\textit{Training Configuration}} \\
\hline
Local epochs ($\tau_{\text{local}}$) & 5 & SGD epochs per round \\
Batch size & 32 & Local training batch size \\
Learning rate ($\eta_0$) & 0.01 & Initial learning rate \\
LR decay & $\eta_0/\sqrt{t}$ & Learning rate schedule \\
Momentum & 0.9 & SGD momentum \\
Total rounds ($T$) & 200 & Maximum training rounds \\
\hline
\multicolumn{3}{c}{\textit{Aggregation Parameters}} \\
\hline
Spatial regularization ($\lambda$) & 0.1 & Inter-zone consensus weight \\
Fairness parameter ($\alpha_{\text{fair}}$) & 0.5 & Fairness adjustment strength \\
Staleness threshold ($\tau_{\max}$) & 10 rounds & Maximum allowed staleness \\
Staleness penalty ($\mu$) & 0.1 & Exponential decay factor \\
Momentum ($\eta$) & 0.7 & Correlation matrix update \\
\hline
\multicolumn{3}{c}{\textit{Communication Optimization}} \\
\hline
Compression rate ($\kappa$) & 0.1 & Top-k sparsification (10\%) \\
Quantization bits & 8 & Delta encoding precision \\
Cache threshold ($\tau_{\text{cache}}$) & 0.95 & Layer stability for caching \\
Cache warmup & 15 rounds & Rounds before caching \\
\hline
\multicolumn{3}{c}{\textit{Resource Constraints}} \\
\hline
Device participation rate & 0.7 & Fraction of active devices \\
Deadline ($T_{\text{deadline}}$) & 60s & Per-round time limit \\
Max bandwidth ($B_{\text{max}}$) & 100 Mbps & Total available bandwidth \\
Device memory & 512 MB & Model storage limit \\
Server memory & 2 GB & Zone aggregator limit \\
\hline
\multicolumn{3}{c}{\textit{Data Distribution}} \\
\hline
Intra-zone Dirichlet ($\alpha_{\text{intra}}$) & 0.5 & Within-zone heterogeneity \\
Inter-zone Dirichlet ($\alpha_{\text{inter}}$) & 0.1 & Across-zone heterogeneity \\
Train-test split & 80-20 & Local data partitioning \\
Validation set & 5\% & Per-zone validation data \\
\hline
\multicolumn{3}{c}{\textit{Failure Simulation}} \\
\hline
Device failure rate & 0-0.4 & Random device dropouts \\
Churn rate & 0.2 & Devices joining/leaving \\
Network packet loss & 0.05 & Communication failures \\
Latency variation & ±50ms & Network fluctuation \\
\hline
\end{tabular}
\end{table}



\section{Performance Evaluation} \label{sec:performance}

We evaluate \SYS through extensive experiments on heterogeneous edge deployments, comparing against state-of-the-art federated learning approaches. Our evaluation addresses four key questions: (1) How much does spatial-aware aggregation improve model accuracy? (2) What are the communication savings from hierarchical aggregation? (3) How well does \SYS scale with increasing zones and devices? (4) How robust is the framework to dynamic edge conditions?

\subsection{Experimental Setup}

\textbf{Testbed configuration.} We deploy \SYS on a realistic edge testbed comprising 100 Raspberry Pi 4B devices (4GB RAM, quad-core ARM) distributed across a university campus, organized into 20 logical zones based on building locations. Each zone is served by an NVIDIA Jetson Orin NX edge server (8GB RAM, 384 CUDA cores) connected via WiFi networks. A cloud server (32-core, 128 GB RAM) serves as the global orchestrator. Network latencies range from 5 to 15ms (intra-zone) to 20 to 100ms (inter-zone), emulating real-world edge conditions.

\textbf{Datasets and models.} We evaluate on three datasets with natural spatial heterogeneity:
% \begin{itemize}
% \item \textit{Urban Sensing}: 2.3M images from traffic cameras across 20 intersections for vehicle classification (10 classes), with each zone exhibiting distinct traffic patterns
% \item \textit{Environmental Monitoring}: 890K sensor readings (temperature, humidity, air quality) for pollution prediction, with zone-specific environmental characteristics  
% \item \textit{Healthcare IoT}: 1.5M ECG signals from wearable devices for arrhythmia detection (5 classes), with demographic variations across zones
% \end{itemize}

\begin{itemize}[wide]
\item \textit{CIFAR-100}~\cite{krizhevsky2009learning}: 60,000 32×32 color images across 100 fine-grained classes, partitioned into 20 superclasses (5 classes each) and distributed across zones. Each zone specializes in 2-3 superclasses with 70\% of its data, creating spatial data heterogeneity that mimics real-world scenarios where different locations observe different object distributions.

\item \textit{FEMNIST}~\cite{caldas2018leaf}: Federated Extended MNIST with 805,263 handwritten character images (62 classes) from 3,550 writers. We group writers by simulated geographical regions (20 zones), where writers in the same zone exhibit similar writing styles, creating natural intra-zone similarity while maintaining inter-zone diversity.

\item \textit{Shakespeare}~\cite{caldas2018leaf}: Text dataset with 16,068 lines from Shakespeare plays, used for next-character prediction. We distribute plays across zones where each zone contains 2-3 complete plays, with some shared plays creating inter-zone correlations. This mimics edge scenarios where different locations process related but distinct text patterns.
\end{itemize}

We train ResNet-18 for image tasks and 3-layer LSTMs for time-series data. Data is distributed non-IID across devices following a Dirichlet distribution with $\alpha=0.5$ within zones and $\alpha=0.1$ across zones, creating strong intra-zone similarity.

\textbf{Baselines.} We compare \SYS against:
\begin{itemize}
\item \textbf{FedAvg}~\cite{mcmahan2017communication}: Standard federated averaging without spatial awareness
\item \textbf{FedProx}~\cite{li2020federated}: Proximal term regularization for heterogeneous devices
\item \textbf{HierFL}~\cite{liu2020client}: Static hierarchical aggregation based on network topology
\item \textbf{ClusterFL}~\cite{sattler2020clustered}: Clustering-based FL in feature space
\item \textbf{FedAMP}~\cite{huang2021personalized}: Personalized FL with attention-based aggregation
\end{itemize}

\textbf{Metrics.} We measure: (i) test accuracy on global and per-zone test sets, (ii) communication cost in MB transmitted, (iii) convergence time in rounds and wall-clock time, (iv) resource utilization across the continuum.



\section{Related Work} \label{sec:related_work}
% \input{Sections/relatedworks}

\section{Conclusion} \label{sec:conclusion}
% \input{Sections/conclusion}

\bibliographystyle{IEEEtran}
\bibliography{ref}

\end{document}
